<!-- 5f0153e2-4d15-44cd-be4e-9a9bdfb5cbec 86462a7e-6a5d-498e-a5d5-ac8c1b24c811 -->
# KB ingestion with Flask compute + Node BullMQ + pgvector

## Decisions

- Compute (extract → normalize → chunk → embed) runs in a Python Flask microservice using `sentence-transformers` with `BAAI/bge-small-en-v1.5` (384 dims).
- Node API owns persistence and search; Python returns chunk texts + 384‑d embeddings.
- Trigger: auto-queue on upload (BullMQ/Redis) with optional manual reingest.

## DB schema (Postgres + pgvector)

- Create migration `apps/api/migrations/001_kb.sql`:
```sql
CREATE EXTENSION IF NOT EXISTS vector;
CREATE TABLE IF NOT EXISTS kb_docs (
  id text PRIMARY KEY,
  title text NOT NULL,
  type text NOT NULL,
  size integer NOT NULL,
  storage_path text NOT NULL,
  status text NOT NULL CHECK (status IN ('uploaded','processing','ingested','failed')),
  error text,
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now()
);
CREATE TABLE IF NOT EXISTS kb_chunks (
  id bigserial PRIMARY KEY,
  doc_id text NOT NULL REFERENCES kb_docs(id) ON DELETE CASCADE,
  chunk_index integer NOT NULL,
  text text NOT NULL,
  embedding vector(384) NOT NULL
);
CREATE INDEX IF NOT EXISTS kb_chunks_embedding_idx
  ON kb_chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

- Run this SQL at API start if `DATABASE_URL` is set.

## Services and packages

- Node (API): `bullmq`, `axios`, `zod`, `pg` (already). Remove Node-side embedding/chunking libs.
- Python (new `apps/ingest_py/`): `Flask`, `sentence-transformers`, `torch` (CPU by default), `PyMuPDF` or `pdfminer.six`, `markdown-it-py`, `beautifulsoup4` (optional), `pydantic`. Serve via `gunicorn`.
- Env:
  - Node: `DATABASE_URL`, `REDIS_URL`, `INGEST_SERVICE_URL` (e.g., `http://ingest:8000`).
  - Python: `MODEL_NAME=BAAI/bge-small-en-v1.5`, `INFERENCE_DEVICE=cpu|cuda`.

## Python Flask microservice (`apps/ingest_py`)

- `app.py` endpoints:
  - `POST /ingest` body: `{ doc_id, title, type, path }` → reads file from shared `data/kb`, extracts → normalizes → chunks (≈1000 tokens, overlap ≈200) → embeds; returns:
    - `{ doc_id, chunks: [{ index, text, embedding: number[] }] }`
  - `POST /embed` body: `{ text }` → `{ embedding: number[] }` for query embedding.
- Embeddings L2-normalized `float32` (384 dims) for cosine.

## Node API repos/controllers (files to add/edit)

- New `apps/api/src/repos/kb.repo.ts`: `insertDoc`, `updateDocStatus`, `insertChunks`.
- Edit `apps/api/src/controllers/kb.controller.ts`: on upload, insert `kb_docs` (`uploaded`, with `storage_path`) and enqueue jobs; add `postIngest(docId)` to re-enqueue.
- Keep `apps/api/src/services/kb.service.ts` for file storage pathing.

## Queue + worker

- New `apps/api/src/queues/ingest.queue.ts`: BullMQ Queue `kb:ingest`.
- New `apps/api/src/workers/ingest.worker.ts`:

1) `updateDocStatus(docId,'processing')`

2) Call Flask `POST /ingest` with `{ doc_id, title, type, path }`

3) `insertChunks(docId, chunks)`; `updateDocStatus(docId,'ingested')`

4) On error: `updateDocStatus(docId,'failed', error)`

- Separate process/script `worker:ingest` to run the worker.

## Search endpoint

- `GET /search?q&top_k=5`:
  - Call Flask `POST /embed` to get query vector, then run pgvector search:
  - `SELECT doc_id, chunk_index, text, 1 - (embedding <=> $1) AS score FROM kb_chunks ORDER BY embedding <=> $1 LIMIT $2;`

## Routing and config

- `apps/api/src/routes/kb.routes.ts`: keep `POST /kb/upload`, `GET /kb/files`, `GET /kb/files/:filename`; add `POST /kb/ingest/:docId`.
- `apps/api/src/index.ts`: wire Redis, run DB bootstrap.
- `docker-compose.yml`: add `ingest` service building `apps/ingest_py`, expose `8000`, share volume `./data/kb:/app/data/kb` with API.

## Minimal API shapes

- Flask `/ingest` response example:
```json
{
  "doc_id": "1697050000-sample.pdf",
  "chunks": [
    { "index": 0, "text": "...", "embedding": [0.01, -0.02] },
    { "index": 1, "text": "...", "embedding": [0.02, -0.01] }
  ]
}
```

- Node worker persistence (illustrative):
```ts
const { data } = await axios.post(`${INGEST_SERVICE_URL}/ingest`, payload);
await insertChunks(docId, data.chunks);
await updateDocStatus(docId, 'ingested');
```


## Acceptance

- Upload PDF/MD/TXT → `kb_docs` progresses uploaded → processing → ingested.
- `kb_chunks` populated with 384‑d vectors; `GET /search` returns relevant chunks.

### To-dos

- [x] Create kb_docs/kb_chunks tables with pgvector(384) and indexes
- [ ] Implement kb.repo.ts with CRUD and insertChunks
- [ ] Implement local bge-small-en embedding service (384-dim)
- [ ] Implement extract.service and chunk.service
- [ ] Add BullMQ queue and ingest worker consuming kb:ingest
- [ ] Modify uploadDocuments to insert kb_docs and enqueue jobs
- [ ] Add optional POST /kb/ingest/:docId endpoint
- [ ] Add GET /search to query pgvector with cosine distance